{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCjRvTxKOQuR"
      },
      "source": [
        "<p style=\"font-size:36px;text-align:center\"> <b>Personalized cancer diagnosis</b> </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylEEdGq_OQuU"
      },
      "source": [
        "<h1>1. Business Problem</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enOmIXotOQuV"
      },
      "source": [
        "<h2>1.1. Description</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8luKZvfOQuX"
      },
      "source": [
        "<p> Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/ </p>\n",
        "<p> Data: Memorial Sloan Kettering Cancer Center (MSKCC)</p>\n",
        "<p> Download training_variants.zip and training_text.zip from Kaggle.</p>\n",
        "\n",
        "<h6> Context:</h6>\n",
        "<p> Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/discussion/35336#198462</p>\n",
        "\n",
        "<h6> Problem statement : </h6>\n",
        "<p> Classify the given genetic variations/mutations based on evidence from text-based clinical literature. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOO0zzCjOQuY"
      },
      "source": [
        "<h2>1.2. Source/Useful Links</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWkAIDUPOQuZ"
      },
      "source": [
        " Some articles and reference blogs about the problem statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ-dIB1COQub"
      },
      "source": [
        "1. https://www.forbes.com/sites/matthewherper/2017/06/03/a-new-cancer-drug-helped-almost-everyone-who-took-it-almost-heres-what-it-teaches-us/#2a44ee2f6b25\n",
        "2. https://www.youtube.com/watch?v=UwbuW7oK8rk\n",
        "3. https://www.youtube.com/watch?v=qxXRKVompI8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QJkDkarOQuc"
      },
      "source": [
        "<h2>1.3. Real-world/Business objectives and constraints.</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCLeDepSOQuc"
      },
      "source": [
        "* No low-latency requirement.\n",
        "* Interpretability is important.\n",
        "* Errors can be very costly.\n",
        "* Probability of a data-point belonging to each class is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AD25TPFOQud"
      },
      "source": [
        "<h1>2. Machine Learning Problem Formulation</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJilLeTgOQuf"
      },
      "source": [
        "<h2>2.1. Data</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAkBG-9JOQuf"
      },
      "source": [
        "<h3>2.1.1. Data Overview</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgJzFEHOQuh"
      },
      "source": [
        "- Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment/data\n",
        "- We have two data files: one conatins the information about the genetic mutations and the other contains the clinical evidence (text) that  human experts/pathologists use to classify the genetic mutations.\n",
        "- Both these data files are have a common column called ID\n",
        "- <p>\n",
        "    Data file's information:\n",
        "    <ul>\n",
        "        <li>\n",
        "        training_variants (ID , Gene, Variations, Class)\n",
        "        </li>\n",
        "        <li>\n",
        "        training_text (ID, Text)\n",
        "        </li>\n",
        "    </ul>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUV7vz1iOQui"
      },
      "source": [
        "<h3>2.1.2. Example Data Point</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4UYmjTIOQuj"
      },
      "source": [
        "<h6>training_variants</h6>\n",
        "<hr>\n",
        "ID,Gene,Variation,Class<br>\n",
        "0,FAM58A,Truncating Mutations,1 <br>\n",
        "1,CBL,W802*,2 <br>\n",
        "2,CBL,Q249E,2 <br>\n",
        "...\n",
        "\n",
        "<h6> training_text</h6>\n",
        "<hr>\n",
        "ID,Text <br>\n",
        "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6). ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3_NkUmXOQuk"
      },
      "source": [
        "<h2>2.2. Mapping the real-world problem to an ML problem</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xF097mOQum"
      },
      "source": [
        "<h3>2.2.1. Type of Machine Learning Problem</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZqA3ItzOQun"
      },
      "source": [
        "<p>\n",
        "    \n",
        "            There are nine different classes a genetic mutation can be classified into => Multi class classification problem\n",
        "   \n",
        "      \n",
        "    \n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uokTUjYOQuo"
      },
      "source": [
        "<h3>2.2.2. Performance Metric</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg0MC7b6OQup"
      },
      "source": [
        "Source: https://www.kaggle.com/c/msk-redefining-cancer-treatment#evaluation\n",
        "\n",
        "Metric(s):\n",
        "* Multi class log-loss\n",
        "* Confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rqQ1-rkOQuq"
      },
      "source": [
        "<h3>2.2.3. Machine Learing Objectives and Constraints</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn3tUsrjOQur"
      },
      "source": [
        "<p> Objective: Predict the probability of each data-point belonging to each of the nine classes.\n",
        "</p>\n",
        "<p> Constraints:\n",
        "</p>\n",
        "* Interpretability\n",
        "* Class probabilities are needed.\n",
        "* Penalize the errors in class probabilites => Metric is Log-loss.\n",
        "* No Latency constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPkaNa1NOQus"
      },
      "source": [
        "<h2>2.3. Train, CV and Test Datasets</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKjsBtT0OQut"
      },
      "source": [
        " Split the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0VVGc0_OQuu"
      },
      "source": [
        "<h1>3. Exploratory Data Analysis</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6_tz3SlAOQuv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7AMKFKcOQu1"
      },
      "source": [
        "<h2>3.1. Reading Data</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBdBb2H3OQu2"
      },
      "source": [
        "<h3>3.1.1. Reading Gene and Variation Data</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "0Os-owW1OQu2",
        "outputId": "50da0c8d-9e57-4834-f038-ddffa4a24f9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'training/training/training_variants'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1995607428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training/training/training_variants'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of data points : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of features : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training/training/training_variants'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('training/training/training_variants')\n",
        "print('Number of data points : ', data.shape[0])\n",
        "print('Number of features : ', data.shape[1])\n",
        "print('Features : ', data.columns.values)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "2e0986d2",
        "outputId": "e124e2a9-6156-40f2-e77a-5a00b30ed155"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to the zip file and the extraction directory\n",
        "zip_file_path = '/content/training.zip'\n",
        "extracted_dir_path = '/content/training'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "print(f\"Files extracted to: {extracted_dir_path}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/training.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-712534486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Unzip the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/training.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VYO0fvbOQu8"
      },
      "source": [
        "<p>\n",
        "    training/training_variants is a comma separated file containing the description of the genetic mutations used for training. <br>\n",
        "    Fields are\n",
        "    <ul>\n",
        "        <li><b>ID : </b>the id of the row used to link the mutation to the clinical evidence</li>\n",
        "        <li><b>Gene : </b>the gene where this genetic mutation is located </li>\n",
        "        <li><b>Variation : </b>the aminoacid change for this mutations </li>\n",
        "        <li><b>Class :</b> 1-9 the class this genetic mutation has been classified on</li>\n",
        "    </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPubFke6OQu9"
      },
      "source": [
        "<h3>3.1.2. Reading Text Data</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i30WJEyOQu9"
      },
      "outputs": [],
      "source": [
        "# note the seprator in this file\n",
        "data_text =pd.read_csv(\"training/training/training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\n",
        "print('Number of data points : ', data_text.shape[0])\n",
        "print('Number of features : ', data_text.shape[1])\n",
        "print('Features : ', data_text.columns.values)\n",
        "data_text.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSy3O7VvOQvC"
      },
      "source": [
        "<h3>3.1.3. Preprocessing of text</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9RpP_0QOQvD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# loading stop words from nltk library\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def nlp_preprocessing(total_text, index, column):\n",
        "    if type(total_text) is not int:\n",
        "        string = \"\"\n",
        "        # replace every special char with space\n",
        "        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n",
        "        # replace multiple spaces with single space\n",
        "        total_text = re.sub('\\s+',' ', total_text)\n",
        "        # converting all the chars into lower-case.\n",
        "        total_text = total_text.lower()\n",
        "\n",
        "        for word in total_text.split():\n",
        "        # if the word is a not a stop word then retain that word from the data\n",
        "            if not word in stop_words:\n",
        "                string += word + \" \"\n",
        "\n",
        "        data_text[column][index] = string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkCyyQm6OQvH"
      },
      "outputs": [],
      "source": [
        "#text processing stage.\n",
        "start_time = time.perf_counter()\n",
        "for index, row in data_text.iterrows():\n",
        "    if type(row['TEXT']) is str:\n",
        "        nlp_preprocessing(row['TEXT'], index, 'TEXT')\n",
        "    else:\n",
        "        print(\"there is no text description for id:\",index)\n",
        "print('Time took for preprocessing the text :',time.perf_counter() - start_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk8gHyNhOQvL"
      },
      "outputs": [],
      "source": [
        "#merging both gene_variations and text data based on ID\n",
        "result = pd.merge(data, data_text,on='ID', how='left')\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ITDIVDOQvQ"
      },
      "outputs": [],
      "source": [
        "result[result.isnull().any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co1v4XZ7OQvU"
      },
      "outputs": [],
      "source": [
        "result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEzTARDBOQvX"
      },
      "outputs": [],
      "source": [
        "result[result['ID']==1109]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ha0M1aOOQvg"
      },
      "source": [
        "<h3>3.1.4. Test, Train and Cross Validation Split</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb_GBIh1OQvi"
      },
      "source": [
        "<h4>3.1.4.1. Splitting data into train, test and cross validation (64:20:16)</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijj-MmMwOQvk"
      },
      "outputs": [],
      "source": [
        "y_true = result['Class'].values\n",
        "result.Gene      = result.Gene.str.replace('\\s+', '_')\n",
        "result.Variation = result.Variation.str.replace('\\s+', '_')\n",
        "\n",
        "# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\n",
        "X_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n",
        "# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\n",
        "train_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWPLX4M9OQvo"
      },
      "source": [
        "<p> We split the data into train, test and cross validation data sets, preserving the ratio of class distribution in the original data set  </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbzQMYZWOQvp"
      },
      "outputs": [],
      "source": [
        "print('Number of data points in train data:', train_df.shape[0])\n",
        "print('Number of data points in test data:', test_df.shape[0])\n",
        "print('Number of data points in cross validation data:', cv_df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQeo-HuDOQvs"
      },
      "source": [
        "<h4>3.1.4.2. Distribution of y_i's in Train, Test and Cross Validation datasets</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC3KErJmOQvt"
      },
      "outputs": [],
      "source": [
        "# it returns a dict, keys as class labels and values as the number of data points in that class\n",
        "train_class_distribution = train_df['Class'].value_counts().sort_index()\n",
        "test_class_distribution = test_df['Class'].value_counts().sort_index()\n",
        "cv_class_distribution = cv_df['Class'].value_counts().sort_index()\n",
        "\n",
        "my_colors = 'rgbkymc'\n",
        "train_class_distribution.plot(kind='bar')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in train data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "# -(train_class_distribution.values): the minus sign will give us in decreasing order\n",
        "sorted_yi = np.argsort(-train_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/train_df.shape[0]*100), 3), '%)')\n",
        "\n",
        "\n",
        "print('-'*80)\n",
        "my_colors = 'rgbkymc'\n",
        "test_class_distribution.plot(kind='bar')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in test data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "# -(train_class_distribution.values): the minus sign will give us in decreasing order\n",
        "sorted_yi = np.argsort(-test_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/test_df.shape[0]*100), 3), '%)')\n",
        "\n",
        "print('-'*80)\n",
        "my_colors = 'rgbkymc'\n",
        "cv_class_distribution.plot(kind='bar')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in cross validation data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "# -(train_class_distribution.values): the minus sign will give us in decreasing order\n",
        "sorted_yi = np.argsort(-train_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]/cv_df.shape[0]*100), 3), '%)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSO9RztaOQvw"
      },
      "source": [
        "<h2>3.2 Prediction using a 'Random' Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7eBGtKAOQvw"
      },
      "source": [
        "<p style=\"font-size:16px\"> In a 'Random' Model, we generate the NINE class probabilites randomly such that they sum to 1. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9quvnOcWOQvx"
      },
      "outputs": [],
      "source": [
        "# This function plots the confusion matrices given y_i, y_i_hat.\n",
        "def plot_confusion_matrix(test_y, predict_y):\n",
        "    C = confusion_matrix(test_y, predict_y)\n",
        "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
        "\n",
        "    A =(((C.T)/(C.sum(axis=1))).T)\n",
        "    #divid each element of the confusion matrix with the sum of elements in that column\n",
        "\n",
        "    # C = [[1, 2],\n",
        "    #     [3, 4]]\n",
        "    # C.T = [[1, 3],\n",
        "    #        [2, 4]]\n",
        "    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
        "    # C.sum(axix =1) = [[3, 7]]\n",
        "    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n",
        "    #                           [2/3, 4/7]]\n",
        "\n",
        "    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n",
        "    #                           [3/7, 4/7]]\n",
        "    # sum of row elements = 1\n",
        "\n",
        "    B =(C/C.sum(axis=0))\n",
        "    #divid each element of the confusion matrix with the sum of elements in that row\n",
        "    # C = [[1, 2],\n",
        "    #     [3, 4]]\n",
        "    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
        "    # C.sum(axix =0) = [[4, 6]]\n",
        "    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n",
        "    #                      [3/4, 4/6]]\n",
        "\n",
        "    labels = [1,2,3,4,5,6,7,8,9]\n",
        "    # representing A in heatmap format\n",
        "    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n",
        "    plt.figure(figsize=(20,7))\n",
        "    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n",
        "    plt.figure(figsize=(20,7))\n",
        "    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "\n",
        "    # representing B in heatmap format\n",
        "    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n",
        "    plt.figure(figsize=(20,7))\n",
        "    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsR6cgBqOQvz"
      },
      "outputs": [],
      "source": [
        "# we need to generate 9 numbers and the sum of numbers should be 1\n",
        "# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n",
        "# ref: https://stackoverflow.com/a/18662466/4084039\n",
        "test_data_len = test_df.shape[0]\n",
        "cv_data_len = cv_df.shape[0]\n",
        "\n",
        "# we create a output array that has exactly same size as the CV data\n",
        "cv_predicted_y = np.zeros((cv_data_len,9))\n",
        "for i in range(cv_data_len):\n",
        "    rand_probs = np.random.rand(1,9)\n",
        "    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
        "print(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y))\n",
        "\n",
        "\n",
        "# Test-Set error.\n",
        "#we create a output array that has exactly same as the test data\n",
        "test_predicted_y = np.zeros((test_data_len,9))\n",
        "for i in range(test_data_len):\n",
        "    rand_probs = np.random.rand(1,9)\n",
        "    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
        "print(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y))\n",
        "\n",
        "predicted_y =  np.argmax(test_predicted_y, axis=1)\n",
        "plot_confusion_matrix(y_test, predicted_y+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIy5AKY7OQv2"
      },
      "source": [
        "<h2>3.3 Univariate Analysis</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glsAAhozOQv3"
      },
      "outputs": [],
      "source": [
        "# code for response coding with Laplace smoothing.\n",
        "# alpha : used for laplace smoothing\n",
        "# feature: ['gene', 'variation']\n",
        "# df: ['train_df', 'test_df', 'cv_df']\n",
        "# algorithm\n",
        "# ----------\n",
        "# Consider all unique values and the number of occurances of given feature in train data dataframe\n",
        "# build a vector (1*9) , the first element = (number of times it occured in class1 + 10*alpha / number of time it occurred in total data+90*alpha)\n",
        "# gv_dict is like a look up table, for every gene it store a (1*9) representation of it\n",
        "# for a value of feature in df:\n",
        "# if it is in train data:\n",
        "# we add the vector that was stored in 'gv_dict' look up table to 'gv_fea'\n",
        "# if it is not there is train:\n",
        "# we add [1/9, 1/9, 1/9, 1/9,1/9, 1/9, 1/9, 1/9, 1/9] to 'gv_fea'\n",
        "# return 'gv_fea'\n",
        "# ----------------------\n",
        "\n",
        "# get_gv_fea_dict: Get Gene varaition Feature Dict\n",
        "def get_gv_fea_dict(alpha, feature, df):\n",
        "    # value_count: it contains a dict like\n",
        "    # print(train_df['Gene'].value_counts())\n",
        "    # output:\n",
        "    #        {BRCA1      174\n",
        "    #         TP53       106\n",
        "    #         EGFR        86\n",
        "    #         BRCA2       75\n",
        "    #         PTEN        69\n",
        "    #         KIT         61\n",
        "    #         BRAF        60\n",
        "    #         ERBB2       47\n",
        "    #         PDGFRA      46\n",
        "    #         ...}\n",
        "    # print(train_df['Variation'].value_counts())\n",
        "    # output:\n",
        "    # {\n",
        "    # Truncating_Mutations                     63\n",
        "    # Deletion                                 43\n",
        "    # Amplification                            43\n",
        "    # Fusions                                  22\n",
        "    # Overexpression                            3\n",
        "    # E17K                                      3\n",
        "    # Q61L                                      3\n",
        "    # S222D                                     2\n",
        "    # P130S                                     2\n",
        "    # ...\n",
        "    # }\n",
        "    value_count = train_df[feature].value_counts()\n",
        "\n",
        "    # gv_dict : Gene Variation Dict, which contains the probability array for each gene/variation\n",
        "    gv_dict = dict()\n",
        "\n",
        "    # denominator will contain the number of time that particular feature occured in whole data\n",
        "    for i, denominator in value_count.items():\n",
        "        # vec will contain (p(yi==1/Gi) probability of gene/variation belongs to perticular class\n",
        "        # vec is 9 diamensional vector\n",
        "        vec = []\n",
        "        for k in range(1,10):\n",
        "            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n",
        "            #         ID   Gene             Variation  Class\n",
        "            # 2470  2470  BRCA1                S1715C      1\n",
        "            # 2486  2486  BRCA1                S1841R      1\n",
        "            # 2614  2614  BRCA1                   M1R      1\n",
        "            # 2432  2432  BRCA1                L1657P      1\n",
        "            # 2567  2567  BRCA1                T1685A      1\n",
        "            # 2583  2583  BRCA1                E1660G      1\n",
        "            # 2634  2634  BRCA1                W1718L      1\n",
        "            # cls_cnt.shape[0] will return the number of rows\n",
        "\n",
        "            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n",
        "\n",
        "            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n",
        "            vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n",
        "\n",
        "        # we are adding the gene/variation to the dict as key and vec as value\n",
        "        gv_dict[i]=vec\n",
        "    return gv_dict\n",
        "\n",
        "# Get Gene variation feature\n",
        "def get_gv_feature(alpha, feature, df):\n",
        "    # print(gv_dict)\n",
        "    #     {'BRCA1': [0.20075757575757575, 0.03787878787878788, 0.068181818181818177, 0.13636363636363635, 0.25, 0.19318181818181818, 0.03787878787878788, 0.03787878787878788, 0.03787878787878788],\n",
        "    #      'TP53': [0.32142857142857145, 0.061224489795918366, 0.061224489795918366, 0.27040816326530615, 0.061224489795918366, 0.066326530612244902, 0.051020408163265307, 0.051020408163265307, 0.056122448979591837],\n",
        "    #      'EGFR': [0.056818181818181816, 0.21590909090909091, 0.0625, 0.068181818181818177, 0.068181818181818177, 0.0625, 0.34659090909090912, 0.0625, 0.056818181818181816],\n",
        "    #      'BRCA2': [0.13333333333333333, 0.060606060606060608, 0.060606060606060608, 0.078787878787878782, 0.1393939393939394, 0.34545454545454546, 0.060606060606060608, 0.060606060606060608, 0.060606060606060608],\n",
        "    #      'PTEN': [0.069182389937106917, 0.062893081761006289, 0.069182389937106917, 0.46540880503144655, 0.075471698113207544, 0.062893081761006289, 0.069182389937106917, 0.062893081761006289, 0.062893081761006289],\n",
        "    #      'KIT': [0.066225165562913912, 0.25165562913907286, 0.072847682119205295, 0.072847682119205295, 0.066225165562913912, 0.066225165562913912, 0.27152317880794702, 0.066225165562913912, 0.066225165562913912],\n",
        "    #      'BRAF': [0.066666666666666666, 0.17999999999999999, 0.073333333333333334, 0.073333333333333334, 0.093333333333333338, 0.080000000000000002, 0.29999999999999999, 0.066666666666666666, 0.066666666666666666],\n",
        "    #      ...\n",
        "    #     }\n",
        "    gv_dict = get_gv_fea_dict(alpha, feature, df)\n",
        "    # value_count is similar in get_gv_fea_dict\n",
        "    value_count = train_df[feature].value_counts()\n",
        "\n",
        "    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n",
        "    gv_fea = []\n",
        "    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n",
        "    # if not we will add [1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9] to gv_fea\n",
        "    for index, row in df.iterrows():\n",
        "        if row[feature] in dict(value_count).keys():\n",
        "            gv_fea.append(gv_dict[row[feature]])\n",
        "        else:\n",
        "            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
        "#             gv_fea.append([-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
        "    return gv_fea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShBOxDTyOQv5"
      },
      "source": [
        "when we caculate the probability of a feature belongs to any particular class, we apply laplace smoothing\n",
        "<li>(numerator + 10\\*alpha) / (denominator + 90\\*alpha) </li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Burwv7NcOQv6"
      },
      "source": [
        "<h3>3.2.1 Univariate Analysis on Gene Feature</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvrxgsHNOQv7"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q1.</b> Gene, What type of feature it is ?</p>\n",
        "<p style=\"font-size:16px;\"><b>Ans.</b> Gene is a categorical variable </p>\n",
        "<p style=\"font-size:18px;\"> <b>Q2.</b> How many categories are there and How they are distributed?</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4VErpbnOQv8"
      },
      "outputs": [],
      "source": [
        "unique_genes = train_df['Gene'].value_counts()\n",
        "print('Number of Unique Genes :', unique_genes.shape[0])\n",
        "# the top 10 genes that occured most\n",
        "print(unique_genes.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5cMcwH-OQwD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Ans: There are\", unique_genes.shape[0] ,\"different categories of genes in the train data, and they are distibuted as follows\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94tkkWJFOQwK"
      },
      "outputs": [],
      "source": [
        "s = sum(unique_genes.values);\n",
        "h = unique_genes.values/s;\n",
        "plt.plot(h, label=\"Histrogram of Genes\")\n",
        "plt.xlabel('Index of a Gene')\n",
        "plt.ylabel('Number of Occurances')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSYysPfLOQwP"
      },
      "outputs": [],
      "source": [
        "c = np.cumsum(h)\n",
        "plt.plot(c,label='Cumulative distribution of Genes')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA0msTHPOQwT"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q3.</b> How to featurize this Gene feature ?</p>\n",
        "\n",
        "<p style=\"font-size:16px;\"><b>Ans.</b>there are two ways we can featurize this variable\n",
        "check out this video: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/\n",
        "<ol><li>One hot Encoding</li><li>Response coding</li></ol></p>\n",
        "<p> We will choose the appropriate featurization based on the ML model we use.  For this problem of multi-class classification with categorical features, one-hot encoding is better for Logistic regression while response coding is better for Random Forests. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcsbuDqcOQwU"
      },
      "outputs": [],
      "source": [
        "#response-coding of the Gene feature\n",
        "# alpha is used for laplace smoothing\n",
        "alpha = 1\n",
        "# train gene feature\n",
        "train_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df))\n",
        "# test gene feature\n",
        "test_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df))\n",
        "# cross validation gene feature\n",
        "cv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CejFANCDOQwb"
      },
      "outputs": [],
      "source": [
        "print(\"train_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:\", train_gene_feature_responseCoding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TiLWR3LOQwq"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding of Gene feature.\n",
        "gene_vectorizer = CountVectorizer()\n",
        "train_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\n",
        "test_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\n",
        "cv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foYJ4tJnOQwt"
      },
      "outputs": [],
      "source": [
        "train_df['Gene'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aohGapoOQwx"
      },
      "outputs": [],
      "source": [
        "gene_vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnMPvg4OOQw0"
      },
      "outputs": [],
      "source": [
        "print(\"train_gene_feature_onehotCoding is converted feature using one-hot encoding method. The shape of gene feature:\", train_gene_feature_onehotCoding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKDkc6TeOQw-"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q4.</b> How good is this gene feature  in predicting y_i?</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ-891DNOQw_"
      },
      "source": [
        "There are many ways to estimate how good a feature is, in predicting y_i. One of the good methods is to build a proper ML model using just this feature. In this case, we will build a logistic regression model using only Gene feature (one hot encoded) to predict y_i."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lcCW-oEOQxA"
      },
      "outputs": [],
      "source": [
        "alpha = [10 ** x for x in range(-5, 1)] # hyperparam for SGD classifier.\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])     Fit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)    Predict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link:\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log_loss', random_state=42)\n",
        "    clf.fit(train_gene_feature_onehotCoding, y_train)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n",
        "    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "clf.fit(train_gene_feature_onehotCoding, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZjuFgxwOQxE"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q5.</b> Is the Gene feature stable across all the data sets (Test, Train, Cross validation)?</p>\n",
        "<p style=\"font-size:16px;\"> <b>Ans.</b> Yes, it is. Otherwise, the CV and Test errors would be significantly more than train error. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjY8VwyuOQxK"
      },
      "outputs": [],
      "source": [
        "print(\"Q6. How many data points in Test and CV datasets are covered by the \", unique_genes.shape[0], \" genes in train dataset?\")\n",
        "\n",
        "test_coverage=test_df[test_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n",
        "cv_coverage=cv_df[cv_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n",
        "\n",
        "print('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\n",
        "print('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8dUhbbOQxN"
      },
      "source": [
        "<h3>3.2.2 Univariate Analysis on Variation Feature</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH8TKwYjOQxO"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q7.</b> Variation, What type of feature is it ?</p>\n",
        "<p style=\"font-size:16px;\"><b>Ans.</b> Variation is a categorical variable </p>\n",
        "<p style=\"font-size:18px;\"> <b>Q8.</b> How many categories are there?</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipBDKwsoOQxP"
      },
      "outputs": [],
      "source": [
        "unique_variations = train_df['Variation'].value_counts()\n",
        "print('Number of Unique Variations :', unique_variations.shape[0])\n",
        "# the top 10 variations that occured most\n",
        "print(unique_variations.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9wWS-NYOQxV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Ans: There are\", unique_variations.shape[0] ,\"different categories of variations in the train data, and they are distibuted as follows\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7NmwrkpOQxk"
      },
      "outputs": [],
      "source": [
        "s = sum(unique_variations.values);\n",
        "h = unique_variations.values/s;\n",
        "plt.plot(h, label=\"Histrogram of Variations\")\n",
        "plt.xlabel('Index of a Variation')\n",
        "plt.ylabel('Number of Occurances')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sWC9-MXOQxo"
      },
      "outputs": [],
      "source": [
        "c = np.cumsum(h)\n",
        "print(c)\n",
        "plt.plot(c,label='Cumulative distribution of Variations')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_jKtHYZOQxt"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q9.</b> How to featurize this Variation feature ?</p>\n",
        "\n",
        "<p style=\"font-size:16px;\"><b>Ans.</b>There are two ways we can featurize this variable\n",
        "check out this video: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/\n",
        "<ol><li>One hot Encoding</li><li>Response coding</li></ol></p>\n",
        "<p> We will be using both these methods to featurize the Variation Feature </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FG82Rp2OQxt"
      },
      "outputs": [],
      "source": [
        "# alpha is used for laplace smoothing\n",
        "alpha = 1\n",
        "# train gene feature\n",
        "train_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df))\n",
        "# test gene feature\n",
        "test_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df))\n",
        "# cross validation gene feature\n",
        "cv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FETgZxHEOQxw"
      },
      "outputs": [],
      "source": [
        "print(\"train_variation_feature_responseCoding is a converted feature using the response coding method. The shape of Variation feature:\", train_variation_feature_responseCoding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruAYi3HwOQxy"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding of variation feature.\n",
        "variation_vectorizer = CountVectorizer()\n",
        "train_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\n",
        "test_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\n",
        "cv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUdraWMYOQx3"
      },
      "outputs": [],
      "source": [
        "print(\"train_variation_feature_onehotEncoded is converted feature using the onne-hot encoding method. The shape of Variation feature:\", train_variation_feature_onehotCoding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhg1JfPtOQx7"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q10.</b> How good is this Variation feature  in predicting y_i?</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjZ4FBNOQx9"
      },
      "source": [
        "Let's build a model just like the earlier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDa2yRKfOQx9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "alpha = [10 ** x for x in range(-5, 1)]\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])     Fit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)    Predict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link:\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log_loss', random_state=42)\n",
        "    clf.fit(train_variation_feature_onehotCoding, y_train)\n",
        "\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n",
        "    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n",
        "\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "clf.fit(train_variation_feature_onehotCoding, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO-gq3cFOQyB"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q11.</b> Is the Variation feature stable across all the data sets (Test, Train, Cross validation)?</p>\n",
        "<p style=\"font-size:16px;\"> <b>Ans.</b> Not sure! But lets be very sure using the below analysis. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBZ5v2yBOQyB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"Q12. How many data points are covered by total \", unique_variations.shape[0], \" genes in test and cross validation data sets?\")\n",
        "test_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\n",
        "cv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\n",
        "print('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\n",
        "print('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J87b3_pEOQyE"
      },
      "source": [
        "<h3>3.2.3 Univariate Analysis on Text Feature</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go1Lmf9tOQyF"
      },
      "source": [
        "1. How many unique words are present in train data?\n",
        "2. How are word frequencies distributed?\n",
        "3. How to featurize text field?\n",
        "4. Is the text feature useful in predicitng y_i?\n",
        "5. Is the text feature stable across train, test and CV datasets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV59LrzTOQyF"
      },
      "outputs": [],
      "source": [
        "# cls_text is a data frame\n",
        "# for every row in data fram consider the 'TEXT'\n",
        "# split the words by space\n",
        "# make a dict with those words\n",
        "# increment its count whenever we see that word\n",
        "\n",
        "def extract_dictionary_paddle(cls_text):\n",
        "    dictionary = defaultdict(int)\n",
        "    for index, row in cls_text.iterrows():\n",
        "        for word in row['TEXT'].split():\n",
        "            dictionary[word] +=1\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8Y15a_1OQyH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "#https://stackoverflow.com/a/1602964\n",
        "def get_text_responsecoding(df):\n",
        "    text_feature_responseCoding = np.zeros((df.shape[0],9))\n",
        "    for i in range(0,9):\n",
        "        row_index = 0\n",
        "        for index, row in df.iterrows():\n",
        "            sum_prob = 0\n",
        "            for word in row['TEXT'].split():\n",
        "                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n",
        "            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n",
        "            row_index += 1\n",
        "    return text_feature_responseCoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnAaW3teOQyL"
      },
      "outputs": [],
      "source": [
        "# building a CountVectorizer with all the words that occured minimum 3 times in train data\n",
        "text_vectorizer = CountVectorizer(min_df=3)\n",
        "train_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n",
        "# getting all the feature names (words)\n",
        "train_text_features= text_vectorizer.get_feature_names_out()\n",
        "\n",
        "# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\n",
        "train_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n",
        "\n",
        "# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\n",
        "text_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n",
        "\n",
        "\n",
        "print(\"Total number of unique words in train data :\", len(train_text_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO_CtltnOQyR"
      },
      "outputs": [],
      "source": [
        "dict_list = []\n",
        "# dict_list =[] contains 9 dictoinaries each corresponds to a class\n",
        "for i in range(1,10):\n",
        "    cls_text = train_df[train_df['Class']==i]\n",
        "    # build a word dict based on the words in that class\n",
        "    dict_list.append(extract_dictionary_paddle(cls_text))\n",
        "    # append it to dict_list\n",
        "\n",
        "# dict_list[i] is build on i'th  class text data\n",
        "# total_dict is buid on whole training text data\n",
        "total_dict = extract_dictionary_paddle(train_df)\n",
        "\n",
        "\n",
        "confuse_array = []\n",
        "for i in train_text_features:\n",
        "    ratios = []\n",
        "    max_val = -1\n",
        "    for j in range(0,9):\n",
        "        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n",
        "    confuse_array.append(ratios)\n",
        "confuse_array = np.array(confuse_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaI8RUu0OQyU"
      },
      "outputs": [],
      "source": [
        "#response coding of text features\n",
        "train_text_feature_responseCoding  = get_text_responsecoding(train_df)\n",
        "test_text_feature_responseCoding  = get_text_responsecoding(test_df)\n",
        "cv_text_feature_responseCoding  = get_text_responsecoding(cv_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYqYSUjoOQyV"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/a/16202486\n",
        "# we convert each row values such that they sum to 1\n",
        "train_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\n",
        "test_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n",
        "cv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UEk7cxGOQyX"
      },
      "outputs": [],
      "source": [
        "# don't forget to normalize every feature\n",
        "train_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n",
        "\n",
        "# we use the same vectorizer that was trained on train data\n",
        "test_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n",
        "# don't forget to normalize every feature\n",
        "test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
        "\n",
        "# we use the same vectorizer that was trained on train data\n",
        "cv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n",
        "# don't forget to normalize every feature\n",
        "cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox21AjDkOQyZ"
      },
      "outputs": [],
      "source": [
        "#https://stackoverflow.com/a/2258273/4084039\n",
        "sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\n",
        "sorted_text_occur = np.array(list(sorted_text_fea_dict.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT0OrfaTOQyb"
      },
      "outputs": [],
      "source": [
        "# Number of words for a given frequency.\n",
        "print(Counter(sorted_text_occur))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sys81mqOQyh"
      },
      "outputs": [],
      "source": [
        "# Train a Logistic regression+Calibration model using text features whicha re on-hot encoded\n",
        "alpha = [10 ** x for x in range(-5, 1)]\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])     Fit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)    Predict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link:\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log_loss', random_state=42)\n",
        "    clf.fit(train_text_feature_onehotCoding, y_train)\n",
        "\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n",
        "    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "clf.fit(train_text_feature_onehotCoding, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_text_feature_onehotCoding, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6AqmMa0OQym"
      },
      "source": [
        "<p style=\"font-size:18px;\"> <b>Q.</b> Is the Text feature stable across all the data sets (Test, Train, Cross validation)?</p>\n",
        "<p style=\"font-size:16px;\"> <b>Ans.</b> Yes, it seems like! </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_cUhp-ZOQyn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_intersec_text(df):\n",
        "    df_text_vec = CountVectorizer(min_df=3)\n",
        "    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n",
        "    df_text_features = df_text_vec.get_feature_names_out()   #  correction ici\n",
        "\n",
        "    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n",
        "    df_text_fea_dict = dict(zip(df_text_features, df_text_fea_counts))\n",
        "    len1 = len(df_text_features)\n",
        "    len2 = len(set(train_text_features) & set(df_text_features))\n",
        "    return len1, len2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjNmkGN9OQyo"
      },
      "outputs": [],
      "source": [
        "# Exemple dutilisation\n",
        "len1, len2 = get_intersec_text(test_df)\n",
        "print(np.round((len2/len1)*100, 3), \"% of words of test data appeared in train data\")\n",
        "\n",
        "len1, len2 = get_intersec_text(cv_df)\n",
        "print(np.round((len2/len1)*100, 3), \"% of words of cross-validation appeared in train data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXWA-CNqOQys"
      },
      "source": [
        "<h1>4. Machine Learning Models</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJzoIyQOQys"
      },
      "outputs": [],
      "source": [
        "#Data preparation for ML models.\n",
        "\n",
        "#Misc. functionns for ML models\n",
        "\n",
        "\n",
        "def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n",
        "    clf.fit(train_x, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x, train_y)\n",
        "    pred_y = sig_clf.predict(test_x)\n",
        "\n",
        "    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n",
        "    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n",
        "    # calculating the number of data points that are misclassified\n",
        "    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n",
        "    plot_confusion_matrix(test_y, pred_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmI-EZ-XOQyv"
      },
      "outputs": [],
      "source": [
        "def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n",
        "    clf.fit(train_x, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(test_x)\n",
        "    return log_loss(test_y, sig_clf_probs, eps=1e-15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m68G02CbOQyw"
      },
      "outputs": [],
      "source": [
        "# this function will be used just for naive bayes\n",
        "# for the given indices, we will print the name of the features\n",
        "# and we will check whether the feature present in the test point text or not\n",
        "def get_impfeature_names(indices, text, gene, var, no_features):\n",
        "    gene_count_vec = CountVectorizer()\n",
        "    var_count_vec = CountVectorizer()\n",
        "    text_count_vec = CountVectorizer(min_df=3)\n",
        "\n",
        "    gene_vec = gene_count_vec.fit(train_df['Gene'])\n",
        "    var_vec  = var_count_vec.fit(train_df['Variation'])\n",
        "    text_vec = text_count_vec.fit(train_df['TEXT'])\n",
        "\n",
        "    fea1_len = len(gene_vec.get_feature_names_out())\n",
        "    fea2_len = len(var_count_vec.get_feature_names_out())\n",
        "\n",
        "    word_present = 0\n",
        "    for i,v in enumerate(indices):\n",
        "        if (v < fea1_len):\n",
        "            word = gene_vec.get_feature_names_out()[v]\n",
        "            yes_no = True if word == gene else False\n",
        "            if yes_no:\n",
        "                word_present += 1\n",
        "                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
        "        elif (v < fea1_len+fea2_len):\n",
        "            word = var_vec.get_feature_names_out()[v-(fea1_len)]\n",
        "            yes_no = True if word == var else False\n",
        "            if yes_no:\n",
        "                word_present += 1\n",
        "                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
        "        else:\n",
        "            word = text_vec.get_feature_names_out()[v-(fea1_len+fea2_len)]\n",
        "            yes_no = True if word in text.split() else False\n",
        "            if yes_no:\n",
        "                word_present += 1\n",
        "                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
        "\n",
        "    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7PORPUOQyy"
      },
      "source": [
        "<p style=\"font-size:24px;text-align:Center\"> <b>Stacking the three types of features </b><p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1VYFJb_OQyy"
      },
      "outputs": [],
      "source": [
        "# merging gene, variance and text features\n",
        "\n",
        "# building train, test and cross validation data sets\n",
        "# a = [[1, 2],\n",
        "#      [3, 4]]\n",
        "# b = [[4, 5],\n",
        "#      [6, 7]]\n",
        "# hstack(a, b) = [[1, 2, 4, 5],\n",
        "#                [ 3, 4, 6, 7]]\n",
        "\n",
        "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
        "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
        "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
        "\n",
        "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\n",
        "train_y = np.array(list(train_df['Class']))\n",
        "\n",
        "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
        "test_y = np.array(list(test_df['Class']))\n",
        "\n",
        "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n",
        "cv_y = np.array(list(cv_df['Class']))\n",
        "\n",
        "\n",
        "train_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\n",
        "test_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\n",
        "cv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n",
        "\n",
        "train_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\n",
        "test_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\n",
        "cv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmSks_ifOQyz"
      },
      "outputs": [],
      "source": [
        "print(\"One hot encoding features :\")\n",
        "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqhwbE-ZOQy1"
      },
      "outputs": [],
      "source": [
        "print(\" Response encoding features :\")\n",
        "print(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\n",
        "print(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\n",
        "print(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwsQlJOpOQy5"
      },
      "source": [
        "<h2>4.1. Base Line Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDbxDY8sOQy6"
      },
      "source": [
        "<h3>4.1.1. Naive Bayes</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75gYdztHOQy7"
      },
      "source": [
        "<h4>4.1.1.1. Hyper parameter tuning</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arRoTgx8OQy7"
      },
      "outputs": [],
      "source": [
        "# find more about Multinomial Naive base function here http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "# -------------------------\n",
        "# default paramters\n",
        "# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
        "\n",
        "# some of methods of MultinomialNB()\n",
        "# fit(X, y[, sample_weight])    Fit Naive Bayes classifier according to X, y\n",
        "# predict(X)    Perform classification on an array of test vectors X.\n",
        "# predict_log_proba(X)  Return log-probability estimates for the test vector X.\n",
        "# -----------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])    Fit the calibrated model\n",
        "# get_params([deep])    Get parameters for this estimator.\n",
        "# predict(X)    Predict the target of new samples.\n",
        "# predict_proba(X)      Posterior probabilities of classification\n",
        "# ----------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = MultinomialNB(alpha=i)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.log10(alpha), cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.xticks(np.log10(alpha))\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozp8TwTvOQy_"
      },
      "source": [
        "<h4>4.1.1.2. Testing the model with best hyper paramters</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBk-DferOQzA"
      },
      "outputs": [],
      "source": [
        "# find more about Multinomial Naive base function here http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "# -------------------------\n",
        "# default paramters\n",
        "# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
        "\n",
        "# some of methods of MultinomialNB()\n",
        "# fit(X, y[, sample_weight])\tFit Naive Bayes classifier according to X, y\n",
        "# predict(X)\tPerform classification on an array of test vectors X.\n",
        "# predict_log_proba(X)\tReturn log-probability estimates for the test vector X.\n",
        "# -----------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "# ----------------------------\n",
        "\n",
        "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "# to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "print(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])\n",
        "plot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU5KUv3HOQzF"
      },
      "source": [
        "<h4>4.1.1.3. Feature Importance, Correctly classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txRgyoK-OQzF",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test_point_index = 1\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "# Use feature_log_prob_ for MultinomialNB\n",
        "indices = np.argsort(-clf.feature_log_prob_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2borO6LOOQzJ"
      },
      "source": [
        "<h4>4.1.1.4. Feature Importance, Incorrectly classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf1lnnLcOQzK",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test_point_index = 100\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_log_prob_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXavTmSOQzO"
      },
      "source": [
        "<h2>4.2. K Nearest Neighbour Classification</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3hlQ7XlOQzO"
      },
      "source": [
        "<h3>4.2.1. Hyper parameter tuning</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmlvjjASOQzP",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "# -------------------------\n",
        "# default parameter\n",
        "# KNeighborsClassifier(n_neighbors=5, weights=uniform, algorithm=auto, leaf_size=30, p=2,\n",
        "# metric=minkowski, metric_params=None, n_jobs=1, **kwargs)\n",
        "\n",
        "# methods of\n",
        "# fit(X, y) : Fit the model using X as training data and y as target values\n",
        "# predict(X):Predict the class labels for the provided data\n",
        "# predict_proba(X):Return probability estimates for the test data X.\n",
        "#-------------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1/\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])    Fit the calibrated model\n",
        "# get_params([deep])    Get parameters for this estimator.\n",
        "# predict(X)    Predict the target of new samples.\n",
        "# predict_proba(X)      Posterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "alpha = [5, 11, 15, 21, 31, 41, 51, 99]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = KNeighborsClassifier(n_neighbors=i)\n",
        "    clf.fit(train_x_responseCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_responseCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_responseCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(cv_x_responseCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(test_x_responseCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SOHvSM2OQzR"
      },
      "source": [
        "<h3>4.2.2. Testing the model with best hyper paramters</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwzl0uRqOQzS"
      },
      "outputs": [],
      "source": [
        "# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "# -------------------------\n",
        "# default parameter\n",
        "# KNeighborsClassifier(n_neighbors=5, weights=uniform, algorithm=auto, leaf_size=30, p=2,\n",
        "# metric=minkowski, metric_params=None, n_jobs=1, **kwargs)\n",
        "\n",
        "# methods of\n",
        "# fit(X, y) : Fit the model using X as training data and y as target values\n",
        "# predict(X):Predict the class labels for the provided data\n",
        "# predict_proba(X):Return probability estimates for the test data X.\n",
        "#-------------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1/\n",
        "#-------------------------------------\n",
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "predict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgemwmdTOQzV"
      },
      "source": [
        "<h3>4.2.3.Sample Query point -1</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN5ixZ-iOQzV"
      },
      "outputs": [],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "test_point_index = 1\n",
        "predicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "neighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\n",
        "print(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\n",
        "print(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FORDzacGOQza"
      },
      "source": [
        "<h3>4.2.4. Sample Query Point-2 </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgshXBUyOQzc"
      },
      "outputs": [],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "test_point_index = 100\n",
        "\n",
        "predicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "neighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\n",
        "print(\"the k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\n",
        "print(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs4v7Fc1OQzj"
      },
      "source": [
        "<h2>4.3. Logistic Regression</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj1NGUOnOQzj"
      },
      "source": [
        "<h3>4.3.1. With Class balancing</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OsvP1C6OQzk"
      },
      "source": [
        "<h4>4.3.1.1. Hyper paramter tuning</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gknPIAJOQzl"
      },
      "outputs": [],
      "source": [
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])     Fit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)    Predict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])    Fit the calibrated model\n",
        "# get_params([deep])    Get parameters for this estimator.\n",
        "# predict(X)    Predict the target of new samples.\n",
        "# predict_proba(X)      Posterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log_loss', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6sk6Z7NOQzp"
      },
      "source": [
        "<h4>4.3.1.2. Testing the model with best hyper paramters</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEeyfXMIOQzp",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])     Fit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)    Predict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9qgkiEwOQzq"
      },
      "source": [
        "<h4>4.3.1.3. Feature Importance</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAN-uQIUOQzr"
      },
      "outputs": [],
      "source": [
        "def get_imp_feature_names(text, indices, removed_ind = []):\n",
        "    word_present = 0\n",
        "    tabulte_list = []\n",
        "    incresingorder_ind = 0\n",
        "    for i in indices:\n",
        "        if i < train_gene_feature_onehotCoding.shape[1]:\n",
        "            tabulte_list.append([incresingorder_ind, \"Gene\", \"Yes\"])\n",
        "        elif i< 18:\n",
        "            tabulte_list.append([incresingorder_ind,\"Variation\", \"Yes\"])\n",
        "        if ((i > 17) & (i not in removed_ind)) :\n",
        "            word = train_text_features[i]\n",
        "            yes_no = True if word in text.split() else False\n",
        "            if yes_no:\n",
        "                word_present += 1\n",
        "            tabulte_list.append([incresingorder_ind,train_text_features[i], yes_no])\n",
        "        incresingorder_ind += 1\n",
        "    print(word_present, \"most importent features are present in our query point\")\n",
        "    print(\"-\"*50)\n",
        "    print(\"The features that are most importent of the \",predicted_cls[0],\" class:\")\n",
        "    print (tabulate(tabulte_list, headers=[\"Index\",'Feature name', 'Present or Not']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UMOPzp5OQzs"
      },
      "source": [
        "<h5>4.3.1.3.1. Correctly Classified point</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrELublWOQzs",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# from tabulate import tabulate\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "clf.fit(train_x_onehotCoding,train_y)\n",
        "test_point_index = 1\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONkuybENOQzu"
      },
      "source": [
        "<h5>4.3.1.3.2. Incorrectly Classified point</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoIr_YdaOQzv"
      },
      "outputs": [],
      "source": [
        "test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqTEV6i4OQzx"
      },
      "source": [
        "<h3>4.3.2. Without Class balancing</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evZWkCjYOQzx"
      },
      "source": [
        "<h4>4.3.2.1. Hyper paramter tuning</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBfRm3hAOQzy"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Dfinir les valeurs d'alpha  tester\n",
        "alpha = [10 ** x for x in range(-6, 1)]\n",
        "cv_log_error_array = []\n",
        "\n",
        "# Boucle sur les diffrentes valeurs d'alpha\n",
        "for i in alpha:\n",
        "    print(\"For alpha =\", i)\n",
        "    #  Correction ici : utiliser loss='log_loss' au lieu de 'log'\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log_loss', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    sig_clf_probs = np.clip(sig_clf_probs, 1e-15, 1 - 1e-15)  #  stabilit numrique\n",
        "\n",
        "    loss = log_loss(cv_y, sig_clf_probs, labels=clf.classes_)\n",
        "    cv_log_error_array.append(loss)\n",
        "    print(\"Log Loss:\", loss)\n",
        "\n",
        "# Visualisation de la performance selon alpha\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array, c='g', marker='o')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array, 3)):\n",
        "    ax.annotate(f\"{txt}\", (alpha[i], cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha values\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "# Choix du meilleur alpha\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "print(\"\\n Best alpha =\", alpha[best_alpha],\n",
        "      \"with CV log loss =\", cv_log_error_array[best_alpha])\n",
        "\n",
        "# Rentraner le modle avec le meilleur alpha\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log_loss', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "# valuation finale sur Train / CV / Test\n",
        "for name, X, y in [\n",
        "    (\"Train\", train_x_onehotCoding, train_y),\n",
        "    (\"CV\", cv_x_onehotCoding, cv_y),\n",
        "    (\"Test\", test_x_onehotCoding, test_y)\n",
        "]:\n",
        "    pred_y = sig_clf.predict_proba(X)\n",
        "    pred_y = np.clip(pred_y, 1e-15, 1 - 1e-15)\n",
        "    loss = log_loss(y, pred_y, labels=clf.classes_)\n",
        "    print(f\"{name} Log Loss: {loss:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1f9PWS5OQzz"
      },
      "source": [
        "<h4>4.3.2.2. Testing model with best hyper parameters</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4orFGr5QOQz0"
      },
      "outputs": [],
      "source": [
        "# read more about SGDClassifier() at\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha],\n",
        "                    penalty='l2',\n",
        "                    loss='log_loss',   #  correction ici\n",
        "                    random_state=42)\n",
        "\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,\n",
        "                                  cv_x_onehotCoding, cv_y,\n",
        "                                  clf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYmCo85cOQz1"
      },
      "source": [
        "<h4>4.3.2.3. Feature Importance, Correctly Classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OEAvXBGOQz1",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Correction ici : 'log_loss' au lieu de 'log'\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha],\n",
        "                    penalty='l2',\n",
        "                    loss='log_loss',   #  CORRECT\n",
        "                    random_state=42)\n",
        "\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "# Assure-toi que ton modle calibr (sig_clf) est bien recalcul aprs clf.fit()\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "test_point_index = 1\n",
        "no_feature = 500\n",
        "\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "\n",
        "print(\"Predicted Class Probabilities:\",\n",
        "      np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]), 4))\n",
        "\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls - 1][:, :no_feature]\n",
        "print(\"-\" * 50)\n",
        "\n",
        "get_impfeature_names(\n",
        "    indices[0],\n",
        "    test_df['TEXT'].iloc[test_point_index],\n",
        "    test_df['Gene'].iloc[test_point_index],\n",
        "    test_df['Variation'].iloc[test_point_index],\n",
        "    no_feature\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIc5XMOnOQz5"
      },
      "source": [
        "<h4>4.3.2.4. Feature Importance, Inorrectly Classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPAiu9VfOQz5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEnJCfulOQz8"
      },
      "source": [
        "<h2>4.4. Linear Support Vector Machines</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQfL7DVoOQz8"
      },
      "source": [
        "<h3>4.4.1. Hyper paramter tuning</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH-oXVB3OQz9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "alpha = [10 ** x for x in range(-5, 3)]\n",
        "cv_log_error_array = []\n",
        "\n",
        "for i in alpha:\n",
        "    print(\"for C =\", i)\n",
        "\n",
        "    # Modle linaire SVM simul via SGDClassifier\n",
        "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2',\n",
        "                        loss='hinge', random_state=42)\n",
        "\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "\n",
        "    #  Correction : suppression de \"eps=1e-15\"\n",
        "    loss = log_loss(cv_y, sig_clf_probs, labels=clf.classes_)\n",
        "\n",
        "    cv_log_error_array.append(loss)\n",
        "    print(\"Log Loss :\", loss)\n",
        "\n",
        "# Visualisation\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array, c='g', marker='o')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array, 3)):\n",
        "    ax.annotate(f\"{txt}\", (alpha[i], cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha (log scale)\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "plt.xscale(\"log\")\n",
        "plt.show()\n",
        "\n",
        "# Slection du meilleur alpha\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "print(\"Best alpha =\", alpha[best_alpha])\n",
        "\n",
        "# Rentrainement final\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha],\n",
        "                    penalty='l2', loss='hinge', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "# valuation finale\n",
        "for name, X, y in [\n",
        "    (\"Train\", train_x_onehotCoding, train_y),\n",
        "    (\"CV\", cv_x_onehotCoding, cv_y),\n",
        "    (\"Test\", test_x_onehotCoding, test_y)\n",
        "]:\n",
        "    pred_y = sig_clf.predict_proba(X)\n",
        "    loss = log_loss(y, pred_y, labels=clf.classes_)  #  sans eps\n",
        "    print(f\"{name} Log Loss: {loss:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrPFI2mIOQz-"
      },
      "source": [
        "<h3>4.4.2. Testing model with best hyper parameters</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE_tZXkQOQz_"
      },
      "outputs": [],
      "source": [
        "# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "# --------------------------------\n",
        "# default parameters\n",
        "# SVC(C=1.0, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001,\n",
        "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
        "\n",
        "# Some of methods of SVM()\n",
        "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
        "# predict(X)\tPerform classification on samples in X.\n",
        "# --------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n",
        "# --------------------------------\n",
        "\n",
        "\n",
        "# clf = SVC(C=alpha[best_alpha],kernel='linear',probability=True, class_weight='balanced')\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PGX_4z9OQ0C"
      },
      "source": [
        "<h3>4.3.3. Feature Importance</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk9RWiIPOQ0C"
      },
      "source": [
        "<h4>4.3.3.1. For Correctly classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWBebbutOQ0D"
      },
      "outputs": [],
      "source": [
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
        "clf.fit(train_x_onehotCoding,train_y)\n",
        "test_point_index = 1\n",
        "# test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Gr_TtqOQ0E"
      },
      "source": [
        "<h4>4.3.3.2. For Incorrectly classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAFOSLWJOQ0E",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo8UpfnJOQ0G"
      },
      "source": [
        "<h2>4.5 Random Forest Classifier</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ9sdCvQOQ0H"
      },
      "source": [
        "<h3>4.5.1. Hyper paramter tuning (With One hot Encoding)</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNV3DyqiOQ0H"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dfinir les hyperparamtres  tester\n",
        "alpha = [100, 200, 500, 1000, 2000]  # nombre d'arbres\n",
        "max_depth = [5, 10]                  # profondeur maximale\n",
        "cv_log_error_array = []\n",
        "\n",
        "# Boucle sur toutes les combinaisons\n",
        "for i in alpha:\n",
        "    for j in max_depth:\n",
        "        print(f\"For n_estimators = {i} and max_depth = {j}\")\n",
        "        clf = RandomForestClassifier(\n",
        "            n_estimators=i,\n",
        "            criterion='gini',\n",
        "            max_depth=j,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "        sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "        sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "\n",
        "        #  Correction : suppression du paramtre eps\n",
        "        loss = log_loss(cv_y, sig_clf_probs, labels=clf.classes_)\n",
        "        cv_log_error_array.append(loss)\n",
        "\n",
        "        print(\"Log Loss :\", loss)\n",
        "\n",
        "# Trouver le meilleur couple (n_estimators, max_depth)\n",
        "best_index = np.argmin(cv_log_error_array)\n",
        "best_n_estimators = alpha[int(best_index / len(max_depth))]\n",
        "best_max_depth = max_depth[int(best_index % len(max_depth))]\n",
        "\n",
        "print(f\"\\n Best n_estimators = {best_n_estimators}, Best max_depth = {best_max_depth}\")\n",
        "\n",
        "# Rentranement avec les meilleurs paramtres\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=best_n_estimators,\n",
        "    criterion='gini',\n",
        "    max_depth=best_max_depth,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "# valuation finale (Train, CV, Test)\n",
        "for name, X, y in [\n",
        "    (\"Train\", train_x_onehotCoding, train_y),\n",
        "    (\"CV\", cv_x_onehotCoding, cv_y),\n",
        "    (\"Test\", test_x_onehotCoding, test_y)\n",
        "]:\n",
        "    pred_y = sig_clf.predict_proba(X)\n",
        "    loss = log_loss(y, pred_y, labels=clf.classes_)\n",
        "    print(f\"{name} Log Loss: {loss:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsQtPEFoOQ0K"
      },
      "source": [
        "<h3>4.5.2. Testing model with best hyper parameters (One Hot Encoding)</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qubuQ89sOQ0L"
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# default parameters\n",
        "# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=gini, max_depth=None, min_samples_split=2,\n",
        "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
        "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False,\n",
        "# class_weight=None)\n",
        "\n",
        "# Some of methods of RandomForestClassifier()\n",
        "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
        "# predict(X)\tPerform classification on samples in X.\n",
        "# predict_proba (X)\tPerform classification on samples in X.\n",
        "\n",
        "# some of attributes of  RandomForestClassifier()\n",
        "# feature_importances_ : array of shape = [n_features]\n",
        "# The feature importances (the higher, the more important the feature).\n",
        "\n",
        "# --------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n",
        "# --------------------------------\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3LW-91MOQ0M"
      },
      "source": [
        "<h3>4.5.3. Feature Importance</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqP0GqsTOQ0N"
      },
      "source": [
        "<h4>4.5.3.1. Correctly Classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRpvSps1OQ0N",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# test_point_index = 10\n",
        "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "test_point_index = 1\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_importances_)\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yobvR_gdOQ0O"
      },
      "source": [
        "<h4>4.5.3.2. Inorrectly Classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clV1VBToOQ0R",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "test_point_index = 100\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actuall Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_importances_)\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxRbJnYaOQ0U"
      },
      "source": [
        "<h3>4.5.3. Hyper paramter tuning (With Response Coding)</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QgvuyJYOQ0U",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparamtres  tester\n",
        "alpha = [10, 50, 100, 200, 500, 1000]\n",
        "max_depth = [2, 3, 5, 10]\n",
        "cv_log_error_array = []\n",
        "\n",
        "# Boucle d'entranement\n",
        "for i in alpha:\n",
        "    for j in max_depth:\n",
        "        print(f\"For n_estimators = {i} and max_depth = {j}\")\n",
        "        clf = RandomForestClassifier(\n",
        "            n_estimators=i,\n",
        "            criterion='gini',\n",
        "            max_depth=j,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        clf.fit(train_x_responseCoding, train_y)\n",
        "        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "        sig_clf.fit(train_x_responseCoding, train_y)\n",
        "        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n",
        "\n",
        "        #  Correction : suppression de eps\n",
        "        loss = log_loss(cv_y, sig_clf_probs, labels=clf.classes_)\n",
        "        cv_log_error_array.append(loss)\n",
        "        print(\"Log Loss :\", loss)\n",
        "\n",
        "# Slection du meilleur modle\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "best_estimators = alpha[int(best_alpha / len(max_depth))]\n",
        "best_depth = max_depth[int(best_alpha % len(max_depth))]\n",
        "print(f\"\\n Best n_estimators = {best_estimators}, max_depth = {best_depth}\")\n",
        "\n",
        "# Rentranement final\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=best_estimators,\n",
        "    criterion='gini',\n",
        "    max_depth=best_depth,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "# valuation sur train / CV / test\n",
        "for name, X, y in [\n",
        "    (\"Train\", train_x_responseCoding, train_y),\n",
        "    (\"CV\", cv_x_responseCoding, cv_y),\n",
        "    (\"Test\", test_x_responseCoding, test_y)\n",
        "]:\n",
        "    pred_y = sig_clf.predict_proba(X)\n",
        "    #  sans eps ici aussi\n",
        "    loss = log_loss(y, pred_y, labels=clf.classes_)\n",
        "    print(f\"{name} Log Loss: {loss:.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOLhihFtOQ0d"
      },
      "source": [
        "<h3>4.5.4. Testing model with best hyper parameters (Response Coding)</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHl1Yx72OQ0e"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)],\n",
        "                             n_estimators=alpha[int(best_alpha/4)],\n",
        "                             criterion='gini',\n",
        "                             max_features='sqrt',\n",
        "                             random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_ouYLkdOQ0k"
      },
      "source": [
        "<h3>4.5.5. Feature Importance</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG8xfciZOQ0l"
      },
      "source": [
        "<h4>4.5.5.1. Correctly Classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qYXQRrgOQ0n"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "\n",
        "test_point_index = 1\n",
        "no_feature = 27\n",
        "predicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_importances_)\n",
        "print(\"-\"*50)\n",
        "for i in indices:\n",
        "    if i<9:\n",
        "        print(\"Gene is important feature\")\n",
        "    elif i<18:\n",
        "        print(\"Variation is important feature\")\n",
        "    else:\n",
        "        print(\"Text is important feature\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kA4Q5NTOQ0q"
      },
      "source": [
        "<h4>4.5.5.2. Incorrectly Classified point</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GfSR036OQ0s"
      },
      "outputs": [],
      "source": [
        "test_point_index = 100\n",
        "predicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_importances_)\n",
        "print(\"-\"*50)\n",
        "for i in indices:\n",
        "    if i<9:\n",
        "        print(\"Gene is important feature\")\n",
        "    elif i<18:\n",
        "        print(\"Variation is important feature\")\n",
        "    else:\n",
        "        print(\"Text is important feature\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVjD8P3FOQ0v"
      },
      "source": [
        "<h2>4.7 Stack the models </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUGHi9VqOQ0x"
      },
      "source": [
        "<h3>4.7.1 testing with hyper parameter tuning</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11MwEoYjOQ0z"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "\n",
        "# --- Modles individuels ---\n",
        "clf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log_loss', class_weight='balanced', random_state=0)\n",
        "clf1.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n",
        "\n",
        "clf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\n",
        "clf2.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n",
        "\n",
        "clf3 = MultinomialNB(alpha=0.001)\n",
        "clf3.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n",
        "\n",
        "# --- Calibration ---\n",
        "sig_clf1.fit(train_x_onehotCoding, train_y)\n",
        "print(\"Logistic Regression :  Log Loss: %0.2f\" % log_loss(cv_y, sig_clf1.predict_proba(cv_x_onehotCoding)))\n",
        "\n",
        "sig_clf2.fit(train_x_onehotCoding, train_y)\n",
        "print(\"Support Vector Machines : Log Loss: %0.2f\" % log_loss(cv_y, sig_clf2.predict_proba(cv_x_onehotCoding)))\n",
        "\n",
        "sig_clf3.fit(train_x_onehotCoding, train_y)\n",
        "print(\"Naive Bayes : Log Loss: %0.2f\" % log_loss(cv_y, sig_clf3.predict_proba(cv_x_onehotCoding)))\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Stacking Classifier ---\n",
        "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "best_alpha = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "for i in alpha:\n",
        "    lr = LogisticRegression(C=i, max_iter=1000)\n",
        "    sclf = StackingClassifier(\n",
        "        classifiers=[sig_clf1, sig_clf2, sig_clf3],\n",
        "        meta_classifier=lr,\n",
        "        use_probas=True\n",
        "    )\n",
        "    sclf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "    log_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n",
        "    print(f\"Stacking Classifier : for alpha={i:.4f}, Log Loss={log_error:.3f}\")\n",
        "\n",
        "    if log_error < best_loss:\n",
        "        best_loss = log_error\n",
        "        best_alpha = i\n",
        "\n",
        "print(f\"\\n Best alpha (C for Logistic Regression) = {best_alpha} with Log Loss = {best_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPBL4VxyOQ02"
      },
      "source": [
        "<h3>4.7.2 testing the model with the best hyper parameters</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBSFqbDhOQ03"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(C=0.1)\n",
        "sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n",
        "sclf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "log_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\n",
        "print(\"Log loss (train) on the stacking classifier :\",log_error)\n",
        "\n",
        "log_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n",
        "print(\"Log loss (CV) on the stacking classifier :\",log_error)\n",
        "\n",
        "log_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\n",
        "print(\"Log loss (test) on the stacking classifier :\",log_error)\n",
        "\n",
        "print(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])\n",
        "plot_confusion_matrix(test_y=test_y, predict_y=sclf.predict(test_x_onehotCoding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWPDZ6BBOQ09"
      },
      "source": [
        "<h3>4.7.3 Maximum Voting classifier </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JQHocgrOQ0-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Refer:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "vclf = VotingClassifier(estimators=[('lr', sig_clf1), ('svc', sig_clf2), ('rf', sig_clf3)], voting='soft')\n",
        "vclf.fit(train_x_onehotCoding, train_y)\n",
        "print(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_onehotCoding)))\n",
        "print(\"Log loss (CV) on the VotingClassifier :\", log_loss(cv_y, vclf.predict_proba(cv_x_onehotCoding)))\n",
        "print(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_onehotCoding)))\n",
        "print(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])\n",
        "plot_confusion_matrix(test_y=test_y, predict_y=vclf.predict(test_x_onehotCoding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JYU4qYrOQ1C"
      },
      "source": [
        "<h1>Additional Things to be done to improve the accuracy, precision and recall</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie6As4kEOQ1C"
      },
      "source": [
        "<ol>\n",
        "    <li> Apply All the models with tf-idf features (Replace CountVectorizer with tfidfVectorizer and run the same cells)</li>\n",
        "    <li> Instead of using all the words in the dataset, use only the top 1000 words based of tf-idf values</li>\n",
        "    <li>Apply Logistic regression with CountVectorizer Features, including both unigrams and bigrams</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZOuDAnYEoE4"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GqIfjZ-EoE4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df = 1, max_features = 1000)\n",
        "train_tfidf_text = tfidf_vectorizer.fit_transform(train_df['TEXT'])\n",
        "train_tfidf_text = normalize(train_tfidf_text, axis = 0)\n",
        "cv_tfidf_text = tfidf_vectorizer.transform(cv_df['TEXT'])\n",
        "cv_tfidf_text = normalize(cv_tfidf_text, axis = 0)\n",
        "test_tfidf_text = tfidf_vectorizer.transform(test_df['TEXT'])\n",
        "test_tfidf_text = normalize(test_tfidf_text, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PVGbhwFEoE4"
      },
      "outputs": [],
      "source": [
        "# merging gene, variance and text features\n",
        "\n",
        "# building train, test and cross validation data sets\n",
        "# a = [[1, 2],\n",
        "#      [3, 4]]\n",
        "# b = [[4, 5],\n",
        "#      [6, 7]]\n",
        "# hstack(a, b) = [[1, 2, 4, 5],\n",
        "#                [ 3, 4, 6, 7]]\n",
        "\n",
        "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
        "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
        "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
        "\n",
        "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_tfidf_text)).tocsr()\n",
        "train_y = np.array(list(train_df['Class']))\n",
        "\n",
        "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_tfidf_text)).tocsr()\n",
        "test_y = np.array(list(test_df['Class']))\n",
        "\n",
        "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_tfidf_text)).tocsr()\n",
        "cv_y = np.array(list(cv_df['Class']))\n",
        "\n",
        "print(\"One hot encoding features :\")\n",
        "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvIXhCWOEoE5"
      },
      "source": [
        "<h1>Naive Bayes Hyper Parameter Tuning</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-arOghiUEoE6"
      },
      "outputs": [],
      "source": [
        "# find more about Multinomial Naive base function here http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "# -------------------------\n",
        "# default paramters\n",
        "# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
        "\n",
        "# some of methods of MultinomialNB()\n",
        "# fit(X, y[, sample_weight])\tFit Naive Bayes classifier according to X, y\n",
        "# predict(X)\tPerform classification on an array of test vectors X.\n",
        "# predict_log_proba(X)\tReturn log-probability estimates for the test vector X.\n",
        "# -----------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "# ----------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = MultinomialNB(alpha=i)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.log10(alpha), cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.xticks(np.log10(alpha))\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxlBVnh5EoFC"
      },
      "source": [
        "<h1>Testing the model with best hyper parameters</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fXZugiZWEoFD"
      },
      "outputs": [],
      "source": [
        "# find more about Multinomial Naive base function here http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "# -------------------------\n",
        "# default paramters\n",
        "# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
        "\n",
        "# some of methods of MultinomialNB()\n",
        "# fit(X, y[, sample_weight])\tFit Naive Bayes classifier according to X, y\n",
        "# predict(X)\tPerform classification on an array of test vectors X.\n",
        "# predict_log_proba(X)\tReturn log-probability estimates for the test vector X.\n",
        "# -----------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/naive-bayes-algorithm-1/\n",
        "# -----------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "# ----------------------------\n",
        "\n",
        "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "# to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "print(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])\n",
        "plot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ7Qa0wkEoFD"
      },
      "source": [
        "<h1>feature importance of correctly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIfzdqf7EoFE"
      },
      "outputs": [],
      "source": [
        "test_point_index = 1\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96gy4wOLEoFH"
      },
      "source": [
        "<h1>feature importance of incorrectly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0B3KfJsEoFS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "test_point_index = 100\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDcbH7qmEoFS"
      },
      "source": [
        "<h1>K Nearest Neighbour</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uJRhcaOEoFS"
      },
      "outputs": [],
      "source": [
        "# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "# -------------------------\n",
        "# default parameter\n",
        "# KNeighborsClassifier(n_neighbors=5, weights=uniform, algorithm=auto, leaf_size=30, p=2,\n",
        "# metric=minkowski, metric_params=None, n_jobs=1, **kwargs)\n",
        "\n",
        "# methods of\n",
        "# fit(X, y) : Fit the model using X as training data and y as target values\n",
        "# predict(X):Predict the class labels for the provided data\n",
        "# predict_proba(X):Return probability estimates for the test data X.\n",
        "#-------------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1/\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "\n",
        "alpha = [5, 11, 15, 21, 31, 41, 51, 99]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = KNeighborsClassifier(n_neighbors=i)\n",
        "    clf.fit(train_x_responseCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_responseCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_responseCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_responseCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_responseCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATOKTfFMEoFT"
      },
      "source": [
        "<h1>testing the model with best hyperparameters</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CHIJszPfEoFT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "# -------------------------\n",
        "# default parameter\n",
        "# KNeighborsClassifier(n_neighbors=5, weights=uniform, algorithm=auto, leaf_size=30, p=2,\n",
        "# metric=minkowski, metric_params=None, n_jobs=1, **kwargs)\n",
        "\n",
        "# methods of\n",
        "# fit(X, y) : Fit the model using X as training data and y as target values\n",
        "# predict(X):Predict the class labels for the provided data\n",
        "# predict_proba(X):Return probability estimates for the test data X.\n",
        "#-------------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1/\n",
        "#-------------------------------------\n",
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "predict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKIpcSRvEoFU"
      },
      "source": [
        "<h1>Sample query point 1</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrkpmpuaEoFU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "test_point_index = 1\n",
        "predicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "neighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\n",
        "print(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\n",
        "print(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqnrh0MxEoFU"
      },
      "source": [
        "<h1>Sample query point 2</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwpJmCTfEoFV"
      },
      "outputs": [],
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "clf.fit(train_x_responseCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_responseCoding, train_y)\n",
        "\n",
        "test_point_index = 100\n",
        "\n",
        "predicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "neighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\n",
        "print(\"the k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\n",
        "print(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRV9PkRoEoFV"
      },
      "source": [
        "<h1>Logistic Regression with class balancing</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aunYa6D2EoFV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvYeIhMEoFW"
      },
      "source": [
        "<h1>Testing the model with best hyper parameter</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok1MkMwbEoFW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3bfRzo7EoFW"
      },
      "source": [
        "<h1>feature importance of correctly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzDk1dMUEoFX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# from tabulate import tabulate\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding,train_y)\n",
        "test_point_index = 1\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDjwZ1eQEoFX"
      },
      "source": [
        "<h1>Incorrectly classified point</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eCCf7D1EoFX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6h6mr4BEoFY"
      },
      "source": [
        "<h1>without class balancing</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v01WLKuEoFY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 1)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbNvNn4cEoFZ"
      },
      "source": [
        "<h1>Testing model with best hyperparameters</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "attGb-EIEoFb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link:\n",
        "#------------------------------\n",
        "\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xFUg9pHEoFc"
      },
      "source": [
        "<h1>feature importance correctly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvsRfKdSEoFc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding,train_y)\n",
        "test_point_index = 1\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRlqqnsdEoFd"
      },
      "source": [
        "<h1>Feature importance incorrectly classified point</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rp5rB85EoFd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF4QRqkqEoFd"
      },
      "source": [
        "<h1>Linear support vector machines</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH1CRMP4EoFd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "# --------------------------------\n",
        "# default parameters\n",
        "# SVC(C=1.0, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001,\n",
        "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
        "\n",
        "# Some of methods of SVM()\n",
        "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
        "# predict(X)\tPerform classification on samples in X.\n",
        "# --------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n",
        "# --------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-5, 3)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for C =\", i)\n",
        "#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n",
        "    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTRPaa2yEoFe"
      },
      "source": [
        "<h1>testing model with best hyper parameters</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL7g2KHVEoFe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "# --------------------------------\n",
        "# default parameters\n",
        "# SVC(C=1.0, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001,\n",
        "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)\n",
        "\n",
        "# Some of methods of SVM()\n",
        "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
        "# predict(X)\tPerform classification on samples in X.\n",
        "# --------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n",
        "# --------------------------------\n",
        "\n",
        "\n",
        "# clf = SVC(C=alpha[best_alpha],kernel='linear',probability=True, class_weight='balanced')\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2sNc3KVEoFe"
      },
      "source": [
        "<h1>feature importance of correctly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0LELMp7DEoFe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
        "clf.fit(train_x_onehotCoding,train_y)\n",
        "test_point_index = 1\n",
        "# test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPvtXIppEoFe"
      },
      "source": [
        "<h1>for incorrectly classified point</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL-qogIdEoFe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "test_point_index = 100\n",
        "no_feature = 500\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVFWIO30EoFf"
      },
      "source": [
        "<h1>Random Forest Classifier </h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMTkzuhsEoFf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# default parameters\n",
        "# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=gini, max_depth=None, min_samples_split=2,\n",
        "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
        "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False,\n",
        "# class_weight=None)\n",
        "\n",
        "# Some of methods of RandomForestClassifier()\n",
        "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
        "# predict(X)\tPerform classification on samples in X.\n",
        "# predict_proba (X)\tPerform classification on samples in X.\n",
        "\n",
        "# some of attributes of  RandomForestClassifier()\n",
        "# feature_importances_ : array of shape = [n_features]\n",
        "# The feature importances (the higher, the more important the feature).\n",
        "\n",
        "# --------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n",
        "# --------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [100,200,500,1000,2000]\n",
        "max_depth = [5, 10]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    for j in max_depth:\n",
        "        print(\"for n_estimators =\", i,\"and max depth = \", j)\n",
        "        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n",
        "        clf.fit(train_x_onehotCoding, train_y)\n",
        "        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "        sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "        sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "'''fig, ax = plt.subplots()\n",
        "features = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\n",
        "ax.plot(features, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[int(i/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfTIxta1EoFf"
      },
      "source": [
        "<h1>testing model with best hyper parameters</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-riUCynEoFf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# --------------------------------\n",
        "# default parameters\n",
        "# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=gini, max_depth=None, min_samples_split=2,\n",
        "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
        "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False,\n",
        "# class_weight=None)\n",
        "\n",
        "# Some of methods of RandomForestClassifier()\n",
        "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
        "# predict(X)\tPerform classification on samples in X.\n",
        "# predict_proba (X)\tPerform classification on samples in X.\n",
        "\n",
        "# some of attributes of  RandomForestClassifier()\n",
        "# feature_importances_ : array of shape = [n_features]\n",
        "# The feature importances (the higher, the more important the feature).\n",
        "\n",
        "# --------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n",
        "# --------------------------------\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9fVadptEoFf"
      },
      "source": [
        "<h1>feature importance of correctly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_EqVOFzEoFg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# test_point_index = 10\n",
        "clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "test_point_index = 1\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actual Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_importances_)\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkeBy-QUEoFg"
      },
      "source": [
        "<h1>feature importance of incorrectly classified points</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac4vCbxsEoFg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "test_point_index = 100\n",
        "no_feature = 100\n",
        "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
        "print(\"Predicted Class :\", predicted_cls[0])\n",
        "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
        "print(\"Actuall Class :\", test_y[test_point_index])\n",
        "indices = np.argsort(-clf.feature_importances_)\n",
        "print(\"-\"*50)\n",
        "get_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4dEKABmEoFg"
      },
      "source": [
        "<h1>Maximum Voting Classifier</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ega2YmQEoFg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#Refer:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "vclf = VotingClassifier(estimators=[('lr', sig_clf1), ('svc', sig_clf2), ('rf', sig_clf3)], voting='soft')\n",
        "vclf.fit(train_x_onehotCoding, train_y)\n",
        "print(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_onehotCoding)))\n",
        "print(\"Log loss (CV) on the VotingClassifier :\", log_loss(cv_y, vclf.predict_proba(cv_x_onehotCoding)))\n",
        "print(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_onehotCoding)))\n",
        "print(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))/test_y.shape[0])\n",
        "plot_confusion_matrix(test_y=test_y, predict_y=vclf.predict(test_x_onehotCoding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffhakmb-EoFh"
      },
      "source": [
        "<h1>Logistic Regression Using Bigrams and Unigrams</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNbX2YvrEoFh"
      },
      "outputs": [],
      "source": [
        "count_vectorizer_bigram = CountVectorizer(ngram_range = (1, 2))\n",
        "train_bow_text_bigram = count_vectorizer_bigram.fit_transform(train_df['TEXT'])\n",
        "train_bow_text_bigram = normalize(train_bow_text_bigram, axis = 0)\n",
        "cv_bow_text_bigram = count_vectorizer_bigram.transform(cv_df['TEXT'])\n",
        "cv_bow_text_bigram = normalize(cv_bow_text_bigram, axis = 0)\n",
        "test_bow_text_bigram = count_vectorizer_bigram.transform(test_df['TEXT'])\n",
        "test_bow_text_bigram = normalize(test_bow_text_bigram, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ0TyamxEoFh"
      },
      "outputs": [],
      "source": [
        "# merging gene, variance and text features\n",
        "\n",
        "# building train, test and cross validation data sets\n",
        "# a = [[1, 2],\n",
        "#      [3, 4]]\n",
        "# b = [[4, 5],\n",
        "#      [6, 7]]\n",
        "# hstack(a, b) = [[1, 2, 4, 5],\n",
        "#                [ 3, 4, 6, 7]]\n",
        "\n",
        "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
        "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
        "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
        "\n",
        "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_bow_text_bigram)).tocsr()\n",
        "train_y = np.array(list(train_df['Class']))\n",
        "\n",
        "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_bow_text_bigram)).tocsr()\n",
        "test_y = np.array(list(test_df['Class']))\n",
        "\n",
        "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_bow_text_bigram)).tocsr()\n",
        "cv_y = np.array(list(cv_df['Class']))\n",
        "\n",
        "print(\"One hot encoding features :\")\n",
        "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45WFjND4EoFh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVXbZt6xOQ1E"
      },
      "source": [
        "                                                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NiuYftYEoFi"
      },
      "outputs": [],
      "source": [
        "# merging gene, variance and text features\n",
        "\n",
        "# building train, test and cross validation data sets\n",
        "# a = [[1, 2],\n",
        "#      [3, 4]]\n",
        "# b = [[4, 5],\n",
        "#      [6, 7]]\n",
        "# hstack(a, b) = [[1, 2, 4, 5],\n",
        "#                [ 3, 4, 6, 7]]\n",
        "\n",
        "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
        "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
        "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
        "\n",
        "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_tfidf_text)).tocsr()\n",
        "train_y = np.array(list(train_df['Class']))\n",
        "\n",
        "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_tfidf_text)).tocsr()\n",
        "test_y = np.array(list(test_df['Class']))\n",
        "\n",
        "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_tfidf_text)).tocsr()\n",
        "cv_y = np.array(list(cv_df['Class']))\n",
        "\n",
        "print(\"One hot encoding features :\")\n",
        "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "KRBdC7cGEoFi",
        "outputId": "15d20481-e98b-4f47-c4ca-1d38f1181c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for alpha = 1e-06\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_x_onehotCoding' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4263889068.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"for alpha =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_onehotCoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0msig_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msig_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_onehotCoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_x_onehotCoding' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "226lVBynEoFj"
      },
      "outputs": [],
      "source": [
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=optimum_alpha, penalty='l2', loss='log', random_state=42)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_f7RAxTEoFj"
      },
      "source": [
        "<h1>Logistic Regression Using Additional Features</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDs6y2SmEoFk"
      },
      "outputs": [],
      "source": [
        "#https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
        "#The code below was taken to give additional features to the dataset that would be useful for prediction of the output\n",
        "train_df['word_count'] = train_df['TEXT'].apply(lambda x: len(str(x).split(\" \")))\n",
        "cv_df['word_count'] = cv_df['TEXT'].apply(lambda x: len(str(x).split(\" \")))\n",
        "test_df['word_count'] = test_df['TEXT'].apply(lambda x: len(str(x).split(\" \")))\n",
        "train_df['char_count'] = train_df['TEXT'].str.len()\n",
        "cv_df['char_count'] = cv_df['TEXT'].str.len()\n",
        "test_df['char_count'] = test_df['TEXT'].str.len()\n",
        "def avg_word(sentence):\n",
        "    words = sentence.split()\n",
        "    return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "train_df['avg_word'] = train_df['TEXT'].apply(lambda x: avg_word(x))\n",
        "cv_df['avg_word'] = cv_df['TEXT'].apply(lambda x: avg_word(x))\n",
        "test_df['avg_word'] = test_df['TEXT'].apply(lambda x: avg_word(x))\n",
        "train_df['numerics'] = train_df['TEXT'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "cv_df['numerics'] = cv_df['TEXT'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "test_df['numerics'] = test_df['TEXT'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZLHlKe9EoFk"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFGEqIPWEoFm"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer()\n",
        "scaler.fit(train_df['word_count'].values.reshape(-1, 1))\n",
        "train_word_count = scaler.transform(train_df['word_count'].values.reshape((-1, 1)))\n",
        "cv_word_count = scaler.transform(cv_df['word_count'].values.reshape(-1, 1))\n",
        "test_word_count = scaler.transform(test_df['word_count'].values.reshape(-1, 1))\n",
        "train_avg_word = scaler.fit_transform(train_df['avg_word'].values.reshape(-1, 1))\n",
        "cv_avg_word = scaler.transform(cv_df['avg_word'].values.reshape(-1, 1))\n",
        "test_avg_word = scaler.transform(test_df['avg_word'].values.reshape(-1, 1))\n",
        "train_numerics = scaler.fit_transform(train_df['numerics'].values.reshape(-1, 1))\n",
        "cv_numerics = scaler.transform(cv_df['numerics'].values.reshape(-1, 1))\n",
        "test_numerics = scaler.transform(test_df['numerics'].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S5-5RrWEoFm"
      },
      "outputs": [],
      "source": [
        "# merging gene, variance and text features\n",
        "\n",
        "# building train, test and cross validation data sets\n",
        "# a = [[1, 2],\n",
        "#      [3, 4]]\n",
        "# b = [[4, 5],\n",
        "#      [6, 7]]\n",
        "# hstack(a, b) = [[1, 2, 4, 5],\n",
        "#                [ 3, 4, 6, 7]]\n",
        "\n",
        "train_gene_var_onehotCoding = hstack((train_word_count, train_avg_word, train_numerics, train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
        "test_gene_var_onehotCoding = hstack((test_word_count, test_avg_word, test_numerics, test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
        "cv_gene_var_onehotCoding = hstack((cv_word_count, cv_avg_word, cv_numerics, cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
        "\n",
        "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_tfidf_text)).tocsr()\n",
        "train_y = np.array(list(train_df['Class']))\n",
        "\n",
        "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_tfidf_text)).tocsr()\n",
        "test_y = np.array(list(test_df['Class']))\n",
        "\n",
        "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_tfidf_text)).tocsr()\n",
        "cv_y = np.array(list(cv_df['Class']))\n",
        "\n",
        "print(\"One hot encoding features :\")\n",
        "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
        "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKKF_GyFEoFn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "\n",
        "\n",
        "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
        "# ----------------------------\n",
        "# default paramters\n",
        "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=sigmoid, cv=3)\n",
        "#\n",
        "# some of the methods of CalibratedClassifierCV()\n",
        "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(X)\tPredict the target of new samples.\n",
        "# predict_proba(X)\tPosterior probabilities of classification\n",
        "#-------------------------------------\n",
        "# video link:\n",
        "#-------------------------------------\n",
        "\n",
        "alpha = [10 ** x for x in range(-6, 3)]\n",
        "cv_log_error_array = []\n",
        "for i in alpha:\n",
        "    print(\"for alpha =\", i)\n",
        "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
        "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
        "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJYKvC7tEoFp"
      },
      "outputs": [],
      "source": [
        "optimum_alpha = 0.0002\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=optimum_alpha, penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(train_x_onehotCoding, train_y)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(train_x_onehotCoding, train_y)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
        "print('For values of alpha = ', optimum_alpha, \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
        "print('For values of alpha = ', optimum_alpha, \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
        "print('For values of alpha = ', optimum_alpha, \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NNPR0KZaEoFp"
      },
      "outputs": [],
      "source": [
        "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "# ------------------------------\n",
        "# default parameters\n",
        "# SGDClassifier(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None,\n",
        "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5,\n",
        "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
        "\n",
        "# some of methods\n",
        "# fit(X, y[, coef_init, intercept_init, ])\tFit linear model with Stochastic Gradient Descent.\n",
        "# predict(X)\tPredict class labels for samples in X.\n",
        "\n",
        "#-------------------------------\n",
        "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
        "#------------------------------\n",
        "clf = SGDClassifier(class_weight='balanced', alpha=optimum_alpha, penalty='l2', loss='log', random_state=42)\n",
        "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQHM99YTEoFq"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "x = PrettyTable()\n",
        "x.field_names = ['Machine Learning Model', 'Alpha', 'Train Loss', 'CV Loss', 'Test Loss', 'Miss%']\n",
        "x.add_row([\"Naive Bayes\", 0.1, 0.89, 1.28, 1.27, '39.66%'])\n",
        "x.add_row([\"K Nearest Neighbor (KNN)\", 5, 0.49, 1.03, 1.06, '34.45%'])\n",
        "x.add_row([\"Logistic Regression (Balanced)\", 0.001, 0.61, 1.15, 1.11, '36.65%'])\n",
        "x.add_row([\"Logistic Regression (Imbalanced)\", 0.001, 0.63, 1.17, 1.16, '35.33%'])\n",
        "x.add_row([\"Linear Support Vector Machines (SVM)\", 0.01, 0.74, 1.16, 1.15, '38.53%'])\n",
        "x.add_row([\"Random Forest Classifier\", 1000, 0.73, 1.17, 1.17, '37.41%'])\n",
        "x.add_row([\"Stacking Classifier\", '-', 0.67, 1.17, 1.15, '37.14%'])\n",
        "x.add_row([\"Maximum Voting Classifier\", '-', 0.91, 1.25, 1.22, '36.39%'])\n",
        "x.add_row([\"Naive Bayes (Tfidf)\", 0.1, 0.61, 1.20, 1.24, '39.29%'])\n",
        "x.add_row([\"K Nearest Neighbor (Tfidf)\", 5, 0.49, 1.04, 1.06, '34.59%'])\n",
        "x.add_row([\"Logistic Regression (Balanced, Tfidf)\", 0.0001, 0.45, 1.03, 1.02, '35.15%'])\n",
        "x.add_row([\"Logistic Regression (Imbalanced, Tfidf)\", 0.0001, 0.44, 1.05, 1.04, '35.15%' ])\n",
        "x.add_row([\"Linear Support Vector Machines (Tfidf)\", 0.0001, 0.48, 1.07, 1.08, '36.09%'])\n",
        "x.add_row([\"Random Forest Classifier (Tfidf)\", 2000, 0.85, 1.21, 1.18, '43.61%'])\n",
        "x.add_row([\"Maximum Voting Classifier (Tfidf)\", '-', 0.83, 1.20, 1.19, '37.44%'])\n",
        "x.add_row([\"Logistic Regression (Unigram and Bigram)\", 0.01, 0.8, 1.28, 1.23, '35.15%'])\n",
        "x.add_row([\"Logistic Regression (Additional Features)\", 0.0002, 0.51, 1.01, 0.99, '35.9%'])\n",
        "\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uba-xeiEoFq"
      },
      "source": [
        "<h1>Conclusion</h1>\n",
        "\n",
        "1. The best model for the prediction of the type of model is logistic regression model which is balanced and has all the additional features such as the number of words in a text, the character length of each word and so on.\n",
        "\n",
        "2. The cross validation and the test loss for this model is below or very close to 1.0\n",
        "\n",
        "3. Adding Unigram and Bigram features gave a very large matrix exceeding 15 lakh features. However, the performance of the model was increased to some extent.\n",
        "\n",
        "4. Maximum voting classifier was able to do well on the test set but did not come close to the logistic regression model with added features."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PersonalizedCancerDiagnosis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}